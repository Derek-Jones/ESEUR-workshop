ESEUR Workshop - Regression modelling
=====================================
:author:    Derek M. Jones
:copyright: Somebody
:backend:   slidy
:max-width: 45em

Regression modelling
--------------------

{nbsp}

Today's hammer

{nbsp}

Fits an equation to data

* you decide which equation to fit

{nbsp}

Reasons for building a regression model

* understanding: willingness to trade-off prediction accuracy for
simplicity

* prediction: willingness to trade-off understanding for better fit

Updating from a pre-computer world
----------------------------------

image::linear_tests_cheat_sheet.png[]

Generalized linear model
------------------------

{nbsp}

<keyword>glm</keyword> a general solution

* less restrictive requirements on data characteristics
* can change default Normal to handle other error distributions

{nbsp}

Overkill for many real-life problems

* who cares, we are using a computer
* maths is more complicated than ordinary linear regression, <keyword>lm</keyword>
* uses more memory/cpu time than <keyword>lm</keyword>

Straightforward case
--------------------

.Total lines of source code in FreeBSD by days elapsed since the project started.
image::regression-1-Herraiz-BSD-x.jpg[]


rexample[examples/Herraiz-BSD.R]

Importance of domain knowledge
------------------------------

.Number of classes in the Groovy compiler at each release, in days since version 1.0.
image::regression-1-Groovy_change-prop2.jpg[]


rexample[examples/Groovy_change-prop2.R]

How it works
------------

Linear model: simplest form of regression

<equ>\qquad y = \alpha + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_n x_n + \epsilon</equ>

{nbsp}

<phrase>response variable</phrase>, <equ>y</equ>, modelled as a linear combination of

* <phrase>explanatory variables</phrase>, <equ>x_n</equ> (<phrase>prediction variable</phrase> or
<phrase>predictor</phrase> are terms used when emphasizing prediction)

* additive error, <equ>\epsilon</equ>, behavior not accounted for by
explanatory variables, assumed to be independent, Normal
distribution

Linear regression
-----------------

Linear refers to the model parameters, i.e., <equ>\beta</equ>

* <equ>y = \alpha + \beta x^2 + \epsilon</equ>
* <equ>y = \alpha + \beta \log(x) + \epsilon</equ>
* <equ>y = \alpha + \beta_1 x + \beta_2 x^2 + \epsilon</equ>

{nbsp}

The most commonly used regression model

* many real world problems exhibit linear behavior, or a good enough
approximation to it for practical purposes, over their input range

* much easier to fit than non-linear models

** can usually be built with minimal input from the user
** will converge to the unique solution

Code for linear regression
--------------------------

{nbsp}

The equation:

<equ>\qquad E[\mathit{sloc}\] = \alpha + \beta\times\mathit{Number{_}days}</equ>

{nbsp}

is coded as:

[source,R]
-----
BSD_mod=glm(sloc ~ Number_days, data=bsd_info)
-----

Fitted model details
--------------------

[source,R]
-----
summary(BSD_mod)
-----

----
Call:
glm(formula = sloc ~ Number_days, data = kind_bsd)

Deviance Residuals: 
   Min      1Q  Median      3Q     Max  
-82990  -32136   -3609   35389   87324  

Coefficients:
             Estimate Std. Error t value Pr(>|t|)    
(Intercept) 1.139e+05  1.171e+03   97.24   <2e-16 ***
Number_days 3.937e+02  4.205e-01  936.33   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for gaussian family taken to be 1657283104)

    Null deviance: 1.4610e+15  on 4826  degrees of freedom
Residual deviance: 7.9964e+12  on 4825  degrees of freedom
AIC: 116172

Number of Fisher Scoring iterations: 2
----

Fitted equation
---------------

The fitted equation is:

<equ>\qquad \mathit{sloc} = 1.139\times10^5 + 3.937\times10^2 \mathit{Number{_}days}</equ>

{nbsp}

Not a perfect fit between the model and data

* Uncertainty in the values of the model parameters:
+
<equ>\mathit{sloc}  =  (1.139\times10^5\pm1.171\times10^3) + </equ>
<equ>\qquad \qquad \qquad (3.937\times10^2\pm4.205\times10^{-1}) \mathit{Number{_}days}</equ>
////////////
<equ>\begin{eqnarray}
\mathit{sloc} & = & (1.139\times10^5\pm1.171\times10^3) + \\
              &   &   (3.937\times10^2\pm4.205\times10^{-1}) \mathit{Number{_}days}
\end{eqnarray}</equ>
////////////

* Uncertainty caused by the inability of the explanatory variable
used in the model to explain everything:
+
<equ>\mathit{sloc} = 1.139\times10^5 + 3.937\times10^2 \mathit{Number{_}days}</equ>
<equ>\qquad \qquad \qquad	       \pm4.071\times10^4</equ>
////////////
<equ>\begin{eqnarray}
\mathit{sloc} = 1.139\times10^5 & + & 3.937\times10^2 \mathit{Number{_}days} \\
				&   &    \pm4.071\times10^4
\end{eqnarray}</equ>
////////////

Predicting from a fitted model
------------------------------

{nbsp}

[source,R]
-----
BSD_pred=predict(BSD_mod)
lines(bsd_info\$Number_days, BSD_pred, col="red")

# or

BSD_pred=predict(BSD_mod,
		 newdata=list(Number_days=0:4000))

lines(0:4000, BSD_pred, col="red")
-----

The BSD data
------------

.Total lines of source code in FreeBSD by days elapsed since the project started.
image::regression-1-Herraiz-BSD-x.jpg[]


rexample[examples/Herraiz-BSD.R]

Scattered measurement points
----------------------------

.Estimated cost and duration of 73 large Dutch federal IT projects.
image::regression-1-federal_IT-cost_dur.jpg[]


rexample[examples/federal_IT-cost_dur.R]

Confidence intervals
--------------------

{nbsp}

What is the actual fitted model likely to be?

[source,R]
-----
fed_pred=predict(fed_mod, newdata=list(log.IT=1:7),
				se.fit=TRUE)
lines(fed_pred\$fit, col="red")

# 2.5% above and below the fit
lines(fed_pred\$fit+qnorm(0.975)*fed_pred\$se.fit,
					col="green")
lines(fed_pred\$fit+qnorm(0.025)*fed_pred\$se.fit,
					col="green")

# qnorm(0.975) == 1.96,  qnorm(0.025) == -1.96
-----

Two confidence bounds
---------------------

{nbsp}

What is the likely range of values for a new measurement?

{nbsp}

Two sources of uncertainty need to be combined

* uncertainty in the model parameters, i.e., the confidence interval

* variance in the data not explained by the fitted model

[source,R]
-----
MSE=sum(fed_mod\$residuals^2)/(length(fed_mod\$residuals)-2)

pred_se=sqrt(fed_pred\$se.fit^2+MSE)


lines(fed_pred\$fit+1.96*pred_se, col="blue")
lines(fed_pred\$fit-1.96*pred_se, col="blue")
-----

Checking visual trend
---------------------

Visual trend can be deceptive

* use local regression, <rcode>loess()</rcode>, to check visual trend

.For each distinct language, the number of lines committed on Github and the number of questions tagged with that language.
image::regression-1-langpop-corger-nl.jpg[]


rexample[examples/langpop-corger-nl.R]

Trend changes later
-------------------

.Percentage of vulnerabilities detected by developers working a given number of years in security.
image::regression-1-acc-sec-experience.jpg[]


rexample[examples/acc-sec-experience.R]

Almost any data can be fitted
-----------------------------

.Number of updates and fixes in each Linux release between version 2.6.11 and 3.2.
image::regression-1-linux-stable.jpg[]


rexample[examples/linux-stable]

Updates: <keyword>summary</keyword> output
--------------------------

<rcode>Pr(&gt;|t|)</rcode> the p-value for the hypothesis that the
coefficient in the corresponding row is zero,

----
Call:
glm(formula = Fixes ~ Total.Updates, data = cleaned)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-310.60  -223.67     0.48   184.51   525.26  

Coefficients:
              Estimate Std. Error t value Pr(>|t|)   
(Intercept)    356.233    101.522   3.509   0.0016 **
Total.Updates   -4.464      8.478  -0.526   0.6029   
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for gaussian family taken to be 60685.71)

    Null deviance: 1655335  on 28  degrees of freedom
Residual deviance: 1638514  on 27  degrees of freedom
AIC: 405.62

Number of Fisher Scoring iterations: 2
----

Nominal data
------------

Non-numeric data

* compiler flags: <code>O0</code>, <code>O2</code>, <code>O3</code>

Impact of compiler optimization flags on ability to continue
operating correctly when subject to random bit-flips

Fitted model has:

* response variable: percentage of correct benchmark program execution
* explanatory variable: optimization level as the explanatory variable

Call to <keyword>glm</keyword>

[source,R]
-----
bitflip_mod=glm(pass.masked ~ opt_level, data=bitflip)
-----

<keyword>summary</keyword> output
-------------------

----
Call:
glm(formula = pass.masked ~ opt_level, data = bitflip)

Deviance Residuals: 
     Min        1Q    Median        3Q       Max  
-12.6689   -2.8454   -0.3478    4.4017    8.1100  

Coefficients:
             Estimate Std. Error t value Pr(>|t|)    
(Intercept)    28.589      1.825  15.665  < 2e-16 ***
opt_levelO2     9.161      2.581   3.550  0.00112 ** 
opt_levelO3     7.429      2.581   2.878  0.00677 ** 
opt_levelosf   11.642      2.414   4.822 2.74e-05 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for gaussian family taken to be 29.97578)

    Null deviance: 1785.8  on 38  degrees of freedom
Residual deviance: 1049.2  on 35  degrees of freedom
AIC: 249.07

Number of Fisher Scoring iterations: 2
----

As an equation
--------------

Fitted equation is:

<equ>\qquad \mathit{pass.masked} = 28.6 + 9.2\times \mathit{D_{O2}} + 7.4\times \mathit{D_{O3}} + 11.6\times \mathit{D_{osf}}</equ>

<equ>\mathit{D_i}</equ> is known as dummy variables or indicator variable and
has one of two values:

<equ>\qquad \qquad 1\quad\mathrm{optimization flag used} </equ>

<equ>\qquad \mathit{D_i} =</equ>

<equ>\qquad \qquad 0\quad\mathrm{optimization flag not used} </equ>
//////////
<equ>\begin{eqnarray}
  &   &1\quad\mathrm{optimization flag used}\\
D & = & \\
  &   &0\quad\mathrm{optimization flag not used}
\end{eqnarray}</equ>
//////////

<code>O0</code> value is implicit, i.e., factored into intercept: 28.6

{nbsp}

Standard errors in <code>O2</code>/<code>O3</code> large enough
for their respective <routput>Estimate</routput> coefficients to overlap

Curved data
-----------

[source,R]
-----
linux_mod=glm(sloc ~ Number_days,
					data=linux_info)
linux_mod=glm(sloc ~ Number_days+I(Number_days^2),
					data=linux_info)
-----

How much better is a fitted quadratic equation, a fitted cubic, ...?

.Lines of code in every initial release of the Linux kernel since version 1.0.
image::regression-1-linux-DAYLOC.jpg[]


rexample[examples/Linux-DAYLOC.R]

AIC
---

Akaiki Information Criteria, <rcode>AIC()</rcode>, takes into account:

* a measure of how well a model fits the data

* free parameters have to pay their way by providing enough
improvement in a model's fit to the data

----
[1] Degree 1, AIC= 13998.0004739753
[1] Degree 2, AIC= 13674.6883243397
[1] Degree 3, AIC= 13220.8542892188
[1] Degree 4, AIC= 13221.7072389496
----

When AIC of two models differ by:

* less than 2: more or less equivalent

* between 4 and 7: clearly distinguishable

* more than 10: definitely different

Cubic model
-----------

----
Call:
glm(formula = LOC ~ Number_days + I(Number_days^2) + I(Number_days^3), 
    data = latest_version)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-428217   -80061     6503    64889   620500  

Coefficients:
                   Estimate Std. Error t value Pr(>|t|)    
(Intercept)       3.432e+05  2.876e+04  11.935  < 2e-16 ***
Number_days      -3.664e+02  5.144e+01  -7.123 3.79e-12 ***
I(Number_days^2)  8.167e-01  2.456e-02  33.258  < 2e-16 ***
I(Number_days^3) -9.184e-05  3.371e-06 -27.242  < 2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for gaussian family taken to be 23001633959)

    Null deviance: 1.8867e+15  on 494  degrees of freedom
Residual deviance: 1.1294e+13  on 491  degrees of freedom
AIC: 13221

Number of Fisher Scoring iterations: 2
----

Fitting exponential response variable
-------------------------------------

image::regression-1-koomeycomputertrends.jpg[]


[source,R]
-----
A_mod=glm(log(Watts) ~ Year, data=koomey)
-----

Fitted equation can be written as:

* <equ>\log(\mathit{Watts}) = \alpha + \beta \mathit{Year} + \epsilon</equ>
* <equ>\mathit{Watts} = e^{\alpha + \beta \mathit{Year} + \epsilon}</equ>
* <equ>\mathit{Watts} = e^{\alpha}\times e^{\beta \mathit{Year} \times e^{\epsilon}</equ>

rexample[examples/koomeycomputertrendsreleaseversion-v36.R]

Fitting power law response variable
-----------------------------------

image::regression-1-li2018_app_number.jpg[]


[source,R]
-----
A_mod=glm(log(Apps) ~ log(Releases), data=li)
A_mod=glm(log(Apps) ~ log(Releases), data=li,
				 subset=(Releases <= 20))
-----

Fitted equation can be written as:

* <equ>\log(\mathit{Apps}) = \alpha + \beta \log(\mathit{Releases}) + \epsilon</equ>
* <equ>\mathit{Apps} = e^{\alpha + \beta \log(\mathit{Releases}) + \epsilon}</equ>
* <equ>\mathit{Apps} = e^{\alpha}\times \mathit{Releases}^{\beta} \times e^{\epsilon}</equ>

rexample[examples/li2018_app_number.R]

Influential observations and Outliers
-------------------------------------

.Hours to develop software for 29 embedded consumer products and the amount of code they contain.
image::regression-1-lines-per-hour.jpg[]


rexample[examples/lines-per-hour.R]

Model diagnostics
-----------------

[source,R]
-----
embed_mod=glm(sloc ~ Effort, data=embedded_info)
plot(embed_mod)
-----

.Diagnostics of fitted model
image::regression-1-lines-hour-diagnostic.jpg[]


rexample[examples/lines-hour-diagnostic.R]

influenceIndexPlot
------------------

.<keyword>influenceIndexPlot</keyword> for the data.
image::regression-1-lines-hour-influence.jpg[]


rexample[examples/lines-hour-influence.R]

Outlier or changepoint?
-----------------------

.Number of medical devices recalled by the US Food and Drug Administration, in two week bins.  Fitted straight regression line (red, with green confidence bounds) and loess fit (green).
image::regression-1-Alemzadeh-Recalls.jpg[]


rexample[examples/Alemzadeh-Recalls.R]

After 2010 is different
-----------------------

.Fitted model after two observations replaced by mean value (left) and two fitted lines, one up to the end of 2010 and one after 2010.
image::regression-1-Alemzadeh-mininf.jpg[]


rexample[examples/Alemzadeh-mininf.R]

Data to try
-----------

Byte benchmark: byte-dos-app-benchmark.csv

* WP - word processing
* SS - spreadsheet
* Sci.Eng - scientific/engineering
* Cmplr - compiling

C source measurements: c_linelen.csv.xz

C source measurements: c_linetok.csv.xz

Project duration/effort: CSA-duration.csv.xz

Multiple regression
-------------------

{nbsp}

Multiple - more than one explanatory variable:

{nbsp}

<equ>y = \alpha + \beta_1 x_1 + \epsilon</equ>

<equ>y = \alpha + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_n x_n + \epsilon</equ>

<equ>y = \alpha + \beta_1 x_1 \times x_2 + \epsilon</equ>

<equ>y = \alpha + \beta_1 \log (x_1) + \beta_2 x_2 + \cdots + \beta_n \sqrt{x_n} + \epsilon</equ>

Processor & Memory speed
------------------------

.SPECint 2006 performance results for processors running at various clock rates, memory chip frequencies and processor family.
image::regression-1-SPEC-Mhz.jpg[]


rexample[examples/SPEC-Mhz.R]

More the merrier?
-----------------

{nbsp}

SPECint results contain 36 columns

R formula notation

* can specify all columns in the sample as explanatory variables

[source,R]
-----
spec_mod=glm(Result ~ ., data=cint)
-----

{nbsp}

Learn nothing

Overfitting reduces out of band prediction accuracy

Information in <rcode>Processor</rcode> column to create an almost perfect model

Just the important variables
----------------------------

80% of the variance in the data is explained by:

[source,R]
-----
spec_mod=glm(Result ~ Processor.MHz + mem_rate + mem_freq,
					        data=cint)
-----

<rcode>+</rcode> operator specifies that explanatory variables are
added

Equation fitted:

<equ>\qquad \mathit{Result}=-2.4\times 10^1+\mathit{Processor.MHz}7.3\times 10^{-3}+</equ>
<equ>\qquad \qquad \qquad \qquad \mathit{mem{_}rate}2.5\times 10^{-3} + \mathit{mem{_}freq}1.0\times 10^{-2}</equ>

//////////
<equ>\mathit{Result}=-2.4\times 10^1+\mathit{Processor.MHz}7.3\times 10^{-3}+\mathit{mem{_}rate}2.5\times 10^{-3}+\mathit{mem{_}freq}1.0\times 10^{-2}</equ>
//////////

SPEC summary output
-------------------

----
Call:
glm(formula = Result ~ Processor.MHz + mem_rate + mem_freq, data = cint)

Deviance Residuals: 
     Min        1Q    Median        3Q       Max  
-15.4056   -2.4448    0.0914    2.3792   11.9429  

Coefficients:
                Estimate Std. Error t value Pr(>|t|)    
(Intercept)   -2.761e+01  8.388e-01  -32.92   <2e-16 ***
Processor.MHz  7.282e-03  2.496e-04   29.18   <2e-16 ***
mem_rate       2.453e-03  3.912e-05   62.70   <2e-16 ***
mem_freq       1.363e-02  5.524e-04   24.68   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for gaussian family taken to be 14.53335)

    Null deviance: 109700  on 1345  degrees of freedom
Residual deviance:  19504  on 1342  degrees of freedom
AIC: 7428.3

Number of Fisher Scoring iterations: 2
----

Individual variable contribution
--------------------------------

.Individual contribution of each explanatory variable to the response variable in the fitted model of SPECint performance.
image::regression-1-SPEC-ind-MHz-DDR.jpg[]


[source,R]
-----
library("visreg")
...
t=visreg(spec_mod, plot=FALSE)
plot(t)
-----

rexample[examples/SPEC-ind-MHz-DDR.R]

Residuals
---------

.Residual of the fitted model of SPECint performance.
image::regression-1-SPEC-resid-MHz-DDR.jpg[]


rexample[examples/SPEC-resid-MHz-DDR.R]

Quadratic SPEC model
--------------------

[source,R]
-----
spec_mod=glm(Result ~ Processor.MHz + I(Processor.MHz^2)+
			mem_rate + I(mem_rate^2)+
			  mem_freq
                        , data=cint)
-----

.Individual contribution of each explanatory variable to the response variable in a quadratic model of SPECint performance.
image::regression-1-SPEC-quad-MHz-DDR.jpg[]


rexample[examples/SPEC-quad-MHz-DDR.R]

Updating a fitted model
-----------------------

Add/remove a new column to/from an existing model

[source,R]
-----
ecc_spec_mod=update(spec_mod, . ~ . + ecc)

ecc_spec_mod=update(spec_mod, . ~ . - mem_freq)
-----

Formula notation
----------------

.Symbols that can be used within a formula to express relationships between explanatory variables.
[label="feat-mean-var-skew", width="75%", cols="^s,<6d"]
[options="header", frame="topbot", grid="none", align="center"]
|===============================
|Operator| Effect
|+| causes both of its operands to be included in the equation.
|:| denotes an interaction between its operands, e.g., <rcode>a:b</rcode> or  <rcode>a:b:c</rcode>.
|*| denotes all possible combinations of <rcode>+</rcode> and <rcode>:</rcode> operators, e.g., <rcode>a*b</rcode> is equivalent to  <rcode>a+b+a:b</rcode>.
|^| denotes all interactions to a specific degree, e.g., <rcode>(a+b+c)^2</rcode> is equivalent to  <rcode>a+b+c+a:b+a:c+b:c</rcode>.
|.| denotes all variables in the data-frame specified in the <roption>data</roption> argument except the response variable.
|-| specifies that the right operand is removed from the equation, e.g., <rcode>a*b-a</rcode> is equivalent to  <rcode>b+a:b</rcode>.
|-1| specifies that an intercept is not to be fitted (many regression fitting functions implicitly include an intercept)
|I()| ...
|===============================

Interaction and shared contribution
-----------------------------------

.Diagram illustrating shared and non-shared contributions made by two variables in explaining Y.
image::regression-1-shared-contrib.jpg[]


rexample[examples/shared-contrib.R]

Two models or one?
------------------

.Estimated and actual effort broken down by communication frequency, along with individually straight fitted lines.
image::regression-1-simula_0a.jpg[]


rexample[examples/simula_0a.R]

Interacting variables
---------------------

Simultaneously fit two straight lines to the data

[source,R]
-----
sim_mod=glm(Actual ~ Estimated+Estimated:Communication,
						data=sim)
-----

Fitted equation:

<equ>\qquad \mathit{Actual} = -270.1+1.18\mathit{Estimated}+0.51\mathit{Estimated}\times \mathit{D}</equ>
<equ>\qquad \qquad \qquad =  -270.1+(1.18+0.51 \mathit{D})\mathit{Estimated}</equ>

<equ>\qquad \qquad 1\quad\mathrm{daily communication} </equ>

<equ>\qquad \mathit{D} =</equ>

<equ>\qquad \qquad 0\quad\mathrm{not daily communication} </equ>

//////////
<equ>\begin{aligned}
\mathit{Actual} & = & -270.1+1.18\mathit{Estimated}+0.51\mathit{Estimated}\times D \\
                & = & -270.1+(1.18+0.51 D)\mathit{Estimated}
\end{aligned}</equ>

<equ>D = \begin{cases}1& \text{daily communication}\\0& \text{not daily communication}\end{cases}</equ>
//////////

Start complicated
-----------------

[source,R]
-----
library(MASS)

sim_mod=glm(Actual ~ (Estimated+Communication+
				Contract+Complexity)^2,
						data=sim)
simple_sim=stepAIC(sim_mod)
-----

----
Analysis of Deviance Table (Type II tests)

Response: Actual
                        LR Chisq Df Pr(>Chisq)    
Estimated                 45.879  1  1.258e-11 ***
Communication             17.272  1  3.240e-05 ***
Contract                   6.767  3    0.07971 .  
Complexity                 1.543  1    0.21423    
Estimated:Communication    2.546  1    0.11060    
Communication:Contract     5.020  3    0.17034    
Contract:Complexity        3.197  2    0.20224    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
----

After following the numbers
---------------------------

[source,R]
-----
sim_mod=glm(Actual ~ Estimated+Communication+
				Communication:Contract,
					data=sim)
-----

Fitted equation is:

<equ>\mathit{Actual} = -274.8+1.21\mathit{Estimated} + 2625\times !\mathit{D} +</equ>
<equ>\qquad \qquad \qquad \qquad \mathit{C_{fp}}(1862\times !\mathit{D} - 197.6\times \mathit{D}) +</equ>
<equ>\qquad \qquad \qquad \qquad \mathit{C_{tp}}(-2270\times !\mathit{D} - 462.2\times \mathit{D}) +</equ>
<equ>\qquad \qquad \qquad \qquad \mathit{C_{ot}}(-2298\times !\mathit{D} - 234.3\times \mathit{D})</equ>
////////////////
<equ>\begin{aligned}
\mathit{Actual} = -274.8+1.21\mathit{Estimated} + 2625\times !\mathit{D} & + & \\
					&   & \mathit{C_{fp}}(1862\times !\mathit{D} - 197.6\times \mathit{D}) + \\
					&   & \mathit{C_{tp}}(-2270\times !\mathit{D} - 462.2\times \mathit{D}) + \\
					&   & \mathit{C_{ot}}(-2298\times !\mathit{D} - 234.3\times \mathit{D})
\end{aligned}</equ>
////////////////

* <equ>\mathit{C_{fp}}</equ> 0/1 is a fixed price contract
* <equ>\mathit{C_{tp}}</equ> 0/1 is a target price contract
* <equ>\mathit{C_{ot}}</equ> 0/1 other kind of contract

Data to try
-----------

Byte benchmark: byte-dos-app-benchmark.csv

* WP - word processing
* SS - spreadsheet
* Sci.Eng - scientific/engineering
* Cmplr - compiling

Growth of PC-Lint: pclint.csv.xz

